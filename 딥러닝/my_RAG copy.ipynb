{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f51608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1040ce28",
   "metadata": {},
   "source": [
    "1. 데이터 로드\n",
    "2. 텍스트 분할\n",
    "3. 인덱싱(임베딩 후 인덱싱)\n",
    "4. 검색(retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "199cfd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89845cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# import ocrmypdf\n",
    "# from PIL import Image\n",
    "# Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "# def lower_pdf_resolution(input_pdf_path, output_pdf_path, dpi=400):\n",
    "#     try:\n",
    "#         gs_command = [\n",
    "#             \"gswin64c.exe\", # 또는 윈도우에서는 \"gswin64c.exe\" 등 Ghostscript 실행 파일 경로\n",
    "#             \"-sDEVICE=pdfwrite\",\n",
    "#             \"-dCompatibilityLevel=1.4\",\n",
    "#             f\"-dColorImageResolution={dpi}\",\n",
    "#             f\"-dGrayImageResolution={dpi}\",\n",
    "#             f\"-dMonoImageResolution={dpi}\",\n",
    "#             \"-dDownsampleColorImages=true\",\n",
    "#             \"-dDownsampleGrayImages=true\",\n",
    "#             \"-dDownsampleMonoImages=true\",\n",
    "#             \"-dNOPAUSE\",\n",
    "#             \"-dBATCH\",\n",
    "#             f\"-sOutputFile={output_pdf_path}\",\n",
    "#             input_pdf_path\n",
    "#         ]\n",
    "#         subprocess.run(gs_command, check=True)\n",
    "#         print(f\"PDF 해상도 조정 완료: {output_pdf_path} (목표 DPI: {dpi})\")\n",
    "#         return True\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"Ghostscript가 설치되어 있지 않거나 경로에 없습니다. Ghostscript를 설치해주세요.\")\n",
    "#         return False\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Ghostscript 처리 중 오류 발생: {e}\")\n",
    "#         return False\n",
    "\n",
    "# # 예시:\n",
    "\n",
    "# if lower_pdf_resolution(input_file, preprocessed_file, dpi=300):\n",
    "#     ocrmypdf.ocr(\n",
    "#     preprocessed_file,\n",
    "#     result_file,\n",
    "#     language=\"kor\",\n",
    "#     deskew=True,\n",
    "#     force_ocr=True,\n",
    "#     max_image_mpixels=None,\n",
    "#     image_dpi=300\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbbc7195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_pdf = \"datasets/manual.pdf\"\n",
    "\n",
    "input_pdf = \"datasets/여비산정기준표.pdf\"\n",
    "preprocessed_file = \"datasets/ocr_output.pdf\"\n",
    "result_file = \"datasets/ocr_output_result.pdf\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17bff288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode\n",
    "\n",
    "pipeline_options = PdfPipelineOptions(\n",
    "    do_ocr=True,\n",
    "    do_table_structure = True,\n",
    "    do_text_extraction=True,\n",
    ")\n",
    "pipeline_options.table_structure_options.mode = TableFormerMode.ACCURATE\n",
    "\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")\n",
    "\n",
    "doc = doc_converter.convert(input_pdf)\n",
    "\n",
    "markdown_text = doc.document.export_to_markdown()\n",
    "# print(markdown_text)\n",
    "#읽지 못하는 텍스트와 사진 처리방법 고민"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a72ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdfplumber\n",
    "# import pandas as pd\n",
    "\n",
    "# def convert_pdf_with_pdfplumber_to_markdown_text(input_pdf_path: str) -> str:\n",
    "#     \"\"\"\n",
    "#     PDF의 텍스트와 테이블을 추출하여 하나의 마크다운 문자열로 반환합니다.\n",
    "#     \"\"\"\n",
    "#     markdown_text = \"\"\n",
    "    \n",
    "#     with pdfplumber.open(input_pdf_path) as pdf:\n",
    "#         for page_num, page in enumerate(pdf.pages):\n",
    "#             markdown_text += f\"## Page {page_num + 1}\\n\\n\"\n",
    "\n",
    "#             # 1) 텍스트 추출\n",
    "#             text = page.extract_text()\n",
    "#             if text:\n",
    "#                 markdown_text += text.strip() + \"\\n\\n\"\n",
    "\n",
    "#             # 2) 테이블 추출\n",
    "#             tables = page.extract_tables()\n",
    "#             for table_idx, table in enumerate(tables):\n",
    "#                 try:\n",
    "#                     df = pd.DataFrame(table[1:], columns=table[0])\n",
    "#                     table_md = df.to_markdown(index=False)\n",
    "#                     markdown_text += f\"### Table {page_num + 1}-{table_idx + 1}\\n\\n\"\n",
    "#                     markdown_text += table_md + \"\\n\\n\"\n",
    "#                 except Exception as e:\n",
    "#                     markdown_text += f\"### Table {page_num + 1}-{table_idx + 1} (Error parsing table)\\n\\n\"\n",
    "\n",
    "#             markdown_text += \"---\\n\\n\"\n",
    "    \n",
    "#     return markdown_text\n",
    "\n",
    "# markdown_text = convert_pdf_with_pdfplumber_to_markdown_text(input_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b81bf392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz  # PyMuPDF\n",
    "# import pandas as pd\n",
    "# import pytesseract\n",
    "# from PIL import Image\n",
    "# import io\n",
    "\n",
    "# def convert_pdf_with_fitz_to_markdown_text(input_pdf_path: str) -> str:\n",
    "#     \"\"\"\n",
    "#     fitz(PyMuPDF) + pytesseract를 사용해 PDF 텍스트와 이미지 기반 테이블을 추출하여 마크다운 형식으로 반환\n",
    "#     \"\"\"\n",
    "#     markdown_text = \"\"\n",
    "#     doc = fitz.open(input_pdf_path)\n",
    "\n",
    "#     for page_num, page in enumerate(doc):\n",
    "#         markdown_text += f\"## Page {page_num + 1}\\n\\n\"\n",
    "\n",
    "#         # 1) 페이지 텍스트 추출\n",
    "#         text = page.get_text()\n",
    "#         if text.strip():\n",
    "#             markdown_text += text.strip() + \"\\n\\n\"\n",
    "\n",
    "#         # 2) 페이지를 이미지로 렌더링\n",
    "#         pix = page.get_pixmap(dpi=300)\n",
    "#         img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
    "\n",
    "#         # 3) pytesseract를 사용해 이미지에서 테이블 추출 시도 (OCR 기반)\n",
    "#         try:\n",
    "#             table_data = pytesseract.image_to_string(img, config=\"--psm 6\", lang=\"kor+eng\")\n",
    "#             if table_data.strip():\n",
    "#                 markdown_text += f\"### Table (OCR from image)\\n\\n\"\n",
    "#                 markdown_text += \"```\\n\" + table_data.strip() + \"\\n```\\n\\n\"\n",
    "#         except Exception as e:\n",
    "#             markdown_text += f\"### Table OCR Error: {str(e)}\\n\\n\"\n",
    "\n",
    "#         markdown_text += \"---\\n\\n\"\n",
    "\n",
    "#     doc.close()\n",
    "#     return markdown_text\n",
    "\n",
    "# markdown_text = convert_pdf_with_fitz_to_markdown_text(input_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c051002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "def table_chunker(pdf_path: str):\n",
    "    chunks = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                # table: 2D 리스트 (헤더 포함)\n",
    "                try:\n",
    "                    df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                except:\n",
    "                    df = pd.DataFrame(table)\n",
    "                md_table = df.to_markdown(index=False)\n",
    "                chunks.append({\n",
    "                    \"text\": md_table,\n",
    "                    \"metadata\": {\n",
    "                        \"type\": \"table\",\n",
    "                        \"page\": page.page_number\n",
    "                    }\n",
    "                })\n",
    "    return chunks\n",
    "\n",
    "# 사용 예시\n",
    "table_chunks = table_chunker(input_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851f6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "\n",
    "chunker = HybridChunker(\n",
    "    tokenizer='BAAI/bge-m3',\n",
    "    max_token_length=256,\n",
    "    overlap=50\n",
    ")\n",
    "\n",
    "chunks = list(chunker.chunk(doc.document))\n",
    "\n",
    "\n",
    "\n",
    "# chunks는 청커로 생성된 문서 조각들\n",
    "# documents = [Document(page_content=chunk.text, metadata={\"source\": chunk.meta}) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faf10664",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_documents = []\n",
    "for chunk in chunks:\n",
    "    text = chunk.text\n",
    "\n",
    "    # page_no를 추출 (여러 Prov 아이템 중 첫 번째의 page_no를 사용하거나, set으로 처리)\n",
    "    page_nos = sorted(\n",
    "        set(\n",
    "            prov.page_no\n",
    "            for item in chunk.meta.doc_items\n",
    "            for prov in item.prov\n",
    "            if hasattr(prov, \"page_no\")\n",
    "        )\n",
    "    ) if hasattr(chunk.meta, \"doc_items\") else []\n",
    "\n",
    "    # 가장 근접한 헤딩(섹션 이름) 추출 (예: 마지막 요소)\n",
    "    headings = chunk.meta.headings if hasattr(chunk.meta, \"headings\") else []\n",
    "    section = headings[-1] if headings else None\n",
    "\n",
    "    metadata = {\n",
    "        \"source\": input_pdf,\n",
    "        \"page_numbers\": page_nos,\n",
    "        \"section\": section\n",
    "    }\n",
    "    # (3) LangChain Document 생성\n",
    "    text_documents.append(Document(page_content=text, metadata=metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56083af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. table_chunks를 Document 형식으로 변환\n",
    "table_documents = [\n",
    "    Document(\n",
    "        page_content=chunk[\"text\"],\n",
    "        metadata={\n",
    "            \"type\": \"table\",\n",
    "            \"page\": chunk[\"metadata\"][\"page\"],\n",
    "            \"source\": input_pdf\n",
    "        }\n",
    "    ) for chunk in table_chunks\n",
    "]\n",
    "\n",
    "# 3. 두 Document 리스트 합치기\n",
    "all_documents = table_documents + text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9dee154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitter = MarkdownTextSplitter(\n",
    "#     chunk_size=800,\n",
    "#     chunk_overlap=100\n",
    "# ) #잘 안돼면 512, 768, 1024 등으로 변경\n",
    "# chunks = splitter.create_documents([markdown_text])\n",
    "\n",
    "# # 3. Document 객체 생성\n",
    "# documents = [Document(page_content=chunk.page_content, metadata={\"source\": input_pdf}) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae8acf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = HuggingFaceEmbeddings(\n",
    "#     model_name='BAAI/bge-m3',\n",
    "#     model_kwargs={'device':device},\n",
    "#     encode_kwargs={'normalize_embeddings':True},\n",
    "# )\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"dragonkue/BGE-m3-ko\",\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True},\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=all_documents,\n",
    "    embedding=embedding_model\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc8f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = vectorstore.similarity_search_with_score(\n",
    "#     query=\"교수가 미국으로 4일 출장을 다녀왔을 때 받을 수 있는 숙박비는?\", \n",
    "#     k=5  # top-k 개수\n",
    "# )\n",
    "\n",
    "# for i, (doc, score) in enumerate(results):\n",
    "#     print(f\"Document {i+1}: (Score: {score:.4f})\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71caadfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "# 모델 초기화\n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "\n",
    "# 상위 3개의 문서 선택\n",
    "compressor = CrossEncoderReranker(model=model, top_n=8)\n",
    "\n",
    "# 문서 압축 검색기 초기화\n",
    "compression_reranker = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c869a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"미국은 가 지역에 속해?\"\n",
    "\n",
    "# # 1. Top-k 유사도 문서 + 점수 가져오기\n",
    "# raw_results = retriever.vectorstore.similarity_search_with_score(query, k=5)\n",
    "# docs, scores = zip(*raw_results)\n",
    "\n",
    "# # 2. 문서 압축 (요약/필터링)\n",
    "# compressed_docs = compressor.compress_documents(list(docs), query=query)\n",
    "\n",
    "# # 3. 출력\n",
    "# for i, (original, score) in enumerate(zip(docs, scores)):\n",
    "#     compressed = next((cd for cd in compressed_docs if cd.page_content in original.page_content), None)\n",
    "#     print(f\"\\nDoc {i+1} (Score: {score:.4f}):\")\n",
    "#     print(f\"- Original:\\n{original.page_content[:300]}\")\n",
    "#     if compressed:\n",
    "#         print(f\"- Compressed:\\n{compressed.page_content[:300]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea8afd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# llm = OllamaLLM(model=\"llama3.1:latest\")\n",
    "# llm = OllamaLLM(model=\"qwen3:latest\")\n",
    "llm = OllamaLLM(model=\"qwen3:4b\")\n",
    "# llm = ChatOpenAI(model_name = \"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If the user asks for a simple answer, summarize the key points.\n",
    "If the question is unrelated to the context in the regulations, respond with \"관련 정보를 찾을 수 없습니다.\"\n",
    "You must answer in Korean.\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer:\n",
    "\"\"\")\n",
    "\n",
    "rag_chain = (\n",
    "    {'context': compression_reranker | format_docs, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83e1df72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's try to figure out how much the professor can get for accommodation costs when they went on a 4-day business trip to the US.\n",
      "\n",
      "First, I need to check the relevant information from the provided tables. The question is about the accommodation cost for a professor traveling to the US. Looking at the \"체 재 비\" (accommodation cost) section, there's a table that categorizes costs based on the region. The \"가\" region includes the US, as per the list: \"가 : 스위스,이스라엘, 아이슬 란드,노르웨이, 덴마크, 아일랜드,오스트레일리아, 뉴질랜드,룩셈부르크,스웨 덴, 영국, 핀란드, 캐나다, 미국, 네덜란드,벨기에, 일본, 오스트리아, 프랑스, 독일,이탈리아, 홍콩,싱가포르, 러시아(모스크바),중국(베이징, 상하이,선전,광저우)\". So the US is under the \"가\" category.\n",
      "\n",
      "Looking at the \"체 재 비\" table, for the \"가\" region, the accommodation cost per night is $220. However, there's also a note about the duration of stay. The first table mentions that if someone stays in the same place for a long time, the costs are reduced. But the question is about a 4-day trip. Wait, but the question is about the total accommodation cost. However, the problem is that the tables might be structured differently. Let me check again.\n",
      "\n",
      "Wait, the \"체 재 비\" table under \"가\" has the accommodation cost as $220 per day. However, the note says that if staying in the same place for a long time, the costs are reduced. But the question is about a 4-day trip. However, the problem is that the tables might not specify whether the $220 is per day or per night. But in the \"체 재 비\" table, the \"일 비\" is (1일당) which is per day. So if the professor stayed 4 days, the total accommodation cost would be 4 days multiplied by $220. But wait, wait, the problem is that the \"가\" region's accommodation cost is $220 per day. But the professor is in the US, which is under \"가\" region. So the total would be 4 days * $220 = $880. But wait, the question is about the total accommodation cost. However, I need to check if there's any other information that might affect this.\n",
      "\n",
      "Wait, looking at the \"공무형편상 부득이한 사유\" (unforeseen circumstances for official business), but the professor's trip is for 4 days, so maybe that's not applicable here. Also, the \"나\" region is for other regions, but the US is in \"가\". So the accommodation cost per day is $220. Therefore, 4 days would be 4 * $220 = $880. However, I need to check the other tables to confirm. \n",
      "\n",
      "Looking at the \"국내여비\" (domestic expense) table, for the \"가\" region, the accommodation cost is $220 per day. So that's consistent. Therefore, the total accommodation cost for 4 days would be $220 * 4 = $880. But wait, the problem is that the \"체 재 비\" table under \"가\" region has the accommodation cost as $220 per day. But the professor is in the US, which is in the \"가\" category. So the answer should be $880. However, I need to check if there's any other factor. \n",
      "\n",
      "Wait, the \"체 재 비\" table also has a note that if staying in the same place for a long time, the cost is reduced. But the question is about a 4-day trip. The note says \"같은 곳에 장기간 체재하는 경우의 일비 및 숙박비는 [---중략---] 감하여 지급한다.\" But since the trip is only 4 days, maybe that doesn't apply here. Therefore, the total accommodation cost would be 4 days * $220 = $880. \n",
      "\n",
      "But wait, let me check the answer again. The answer might be different. Let me check the \"체 재 비\" table again. The \"가\" region has the accommodation cost as $220 per day. So 4 days would be 4 * 220 = 880. So the answer should be $880. But maybe the problem is that the professor is in the US, which is in the \"가\" region, so the answer is correct. However, I need to check if the \"체 재 비\" is per day or per night. The (1일당) is per day, so that's correct. Therefore, the answer is $880.\n",
      "</think>\n",
      "\n",
      "교수가 미국으로 4일 출장을 다녀왔을 때 받을 수 있는 총 숙박비는 **$880**입니다.  \n",
      "\n",
      "**설명:**  \n",
      "- 미국은 \"가\" 지역에 해당하며, 해당 지역의 숙박비는 **$220/일**입니다.  \n",
      "- 4일간의 출장으로 인해 총 숙박비는 $220 × 4 = **$880**입니다.  \n",
      "- \"장기간 체재하는 경우\"에 대한 감면 규정은 4일의 짧은 출장에 해당하지 않으므로 적용되지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "question = \"교수가 미국으로 4일 출장을 다녀왔을 때 받을 수 있는 총 숙박비는?\"\n",
    "result = rag_chain.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb23566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #pdf내의 표를 저장함\n",
    "# from pathlib import Path\n",
    "\n",
    "# def extract_and_export_tables(input_path, output_dir):\n",
    "#     output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     doc_converter = DocumentConverter()\n",
    "    \n",
    "#     conv_res = doc_converter.convert(input_path)\n",
    "#     doc_filename = conv_res.input.file.stem\n",
    "    \n",
    "#     for table_ix, table in enumerate(conv_res.document.tables):\n",
    "#         table_df = table.export_to_dataframe()\n",
    "        \n",
    "#         csv_filename = output_dir / f\"{doc_filename}-table-{table_ix+1}.csv\"\n",
    "#         table_df.to_csv(csv_filename)\n",
    "        \n",
    "#         html_filename = output_dir / f\"{doc_filename}-table-{table_ix+1}.html\"\n",
    "#         with html_filename.open(\"w\") as fp:\n",
    "#             fp.write(table.export_to_html())\n",
    "            \n",
    "#     table_count = len(conv_res.document.tables) \n",
    "    \n",
    "#     return table_count\n",
    "\n",
    "\n",
    "# output_dir = Path(\"output\")\n",
    "# output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# table_count = extract_and_export_tables(filepath, output_dir)\n",
    "# print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4efe79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain_docling.loader import ExportType\n",
    "\n",
    "# filepath = 'datasets/여비산정기준표.pdf'\n",
    "# embedding_model = 'BAAI/bge-m3'\n",
    "# reranking_model = 'Alibaba-NLP/gte-multilingual-reranker-base'\n",
    "# export_type = ExportType.DOC_CHUNKS\n",
    "\n",
    "# prompt = PromptTemplate.from_template(\"\"\"\n",
    "# You are an assistant for question-answering tasks. \n",
    "# Use the following pieces of retrieved context to answer the question.\n",
    "# If the user asks for a simple answer, summarize the key points.\n",
    "# If the question is unrelated to the context in the regulations, respond with \"관련 정보를 찾을 수 없습니다.\"\n",
    "# You must answer in Korean.\n",
    "\n",
    "# #Context: \n",
    "# {context}\n",
    "\n",
    "# #Question:\n",
    "# {question}\n",
    "\n",
    "# #Answer:\n",
    "# \"\"\")\n",
    "\n",
    "# from langchain_docling import DoclingLoader\n",
    "# from docling.chunking import HybridChunker\n",
    "\n",
    "# loader = DoclingLoader(\n",
    "#     file_path = filepath,\n",
    "#     export_type = ExportType.DOC_CHUNKS,\n",
    "#     chunker = HybridChunker(tokenizer=embedding_model)\n",
    "# )\n",
    "\n",
    "# docs = loader.load()\n",
    "\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_milvus import Milvus\n",
    "\n",
    "# MUIVUS_URI = \"./milvus_example.db\"\n",
    "\n",
    "# embedding = HuggingFaceEmbeddings(\n",
    "#     model_name=embedding_model,\n",
    "#     model_kwargs={'device':device,\n",
    "#                   'trust_remote_code':True}\n",
    "# )\n",
    "\n",
    "# vectorstore = Milvus.from_documents(\n",
    "#     documents=docs,\n",
    "#     embedding=embedding,\n",
    "#     collection_name=\"docling_transformer\",\n",
    "#     connection_args={\n",
    "#         \"url\":MUIVUS_URI,\n",
    "#         \"db_name\":\" edu\"\n",
    "#     },\n",
    "#     index_params={\n",
    "#         \"index_type\":\"FLAT\",\n",
    "#         \"metric_type\": \"cosine\"\n",
    "#     },\n",
    "#     drop_old=True\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
