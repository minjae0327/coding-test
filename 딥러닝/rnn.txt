**π“„ RNN ~ Transformer: κ°€μ¤‘μΉ κµ¬μ΅° λ° κ³µμ  μ”μ•½ λ¬Έμ„**

---

### π” RNN(Recurrent Neural Network)

#### β… ν•µμ‹¬ κµ¬μ΅°
- μ…λ ¥: \( x_t \)
- μ€λ‹‰ μƒνƒ: \( h_t = \tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h) \)
- μ¶λ ¥: \( y_t = W_{hy}h_t + b_y \)

#### β… ν•™μµλλ” κ°€μ¤‘μΉ
- \( W_{xh} \): μ…λ ¥ β†’ μ€λ‹‰ (input to hidden)
- \( W_{hh} \): μ΄μ „ μ€λ‹‰ β†’ ν„μ¬ μ€λ‹‰ (hidden to hidden)
- \( b_h \): μ€λ‹‰ λ°”μ΄μ–΄μ¤
- \( W_{hy} \), \( b_y \): μ€λ‹‰ β†’ μ¶λ ¥μΈµ (μ¶λ ¥μΈµ κ°€μ¤‘μΉλ„ ν•™μµλ¨)

#### π”„ κ°€μ¤‘μΉ κ³µμ 
- RNNμ€ ν•λ‚μ RNN μ…€ κµ¬μ΅°λ¥Ό λ¨λ“  μ‹μ μ— λ³µμ‚¬ν•΄μ„ μ‚¬μ©ν•λ” λ°©μ‹
- λ”°λΌμ„ λ¨λ“  μ‹μ  \( t \)μ—μ„ λ™μΌν• \( W_{xh}, W_{hh} \)λ¥Ό μ‚¬μ©
- **μ‹κ°„ μ¶•μ„ λ”°λΌ κ³µμ (time-unrolled weight sharing)**

---

### π”€ Transformer

#### β… μ£Όμ” Weight Sharing λ°©μ‹

1. **π“ μ…λ ¥ μ„λ² λ”© β†” μ¶λ ¥μΈµ κ°€μ¤‘μΉ (Weight Tying)**
   - μ…λ ¥ μ„λ² λ”© ν–‰λ ¬ \( E \)λ¥Ό μ¶λ ¥μΈµμ weightλ΅ μ¬μ‚¬μ©
   - \( P(y_t) = \text{softmax}(E^\top h_t) \)
   - μ΄μ : νλΌλ―Έν„° μ κ°μ† + μ…λ ¥/μ¶λ ¥ μ •λ ¬ κ°•ν™”

2. **π“ λ μ΄μ–΄ κ°€μ¤‘μΉ κ³µμ  (Layer Sharing)**
   - μ: ALBERT λ¨λΈ
   - λ™μΌν• encoder layer νλΌλ―Έν„°λ¥Ό λ¨λ“  λ μ΄μ–΄μ— κ³µμ 
   - μ΄μ : λ¨λΈ ν¬κΈ° κ°μ† + μΌλ°ν™” ν–¥μƒ

---

### β… μ „μ²΄ μ”μ•½ ν‘

| λ¨λΈ       | κ³µμ  λ°©μ‹                        | μ„¤λ… |
|------------|-----------------------------------|------|
| RNN        | μ‹κ°„μ¶• κ°€μ¤‘μΉ κ³µμ                | λ¨λ“  μ‹μ μ—μ„ λ™μΌν• RNN μ…€ μ‚¬μ© |
| Transformer| Embedding-Output weight tying    | μ…λ ¥ μ„λ² λ”© = μ¶λ ¥ weight |
| Transformer| Encoder layer weight sharing     | μΌλ¶€ λ¨λΈμ—μ„ λ μ΄μ–΄ μ „μ²΄ κ³µμ  |


