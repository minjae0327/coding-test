{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78392ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading facial-keypoints-detection.zip to c:\\Users\\minja\\GitHub\\coding-test\\deeplearning\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/76.3M [00:00<?, ?B/s]\n",
      "100%|██████████| 76.3M/76.3M [00:00<00:00, 1.48GB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c facial-keypoints-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec523b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully unzipped facial-keypoints-detection.zip to datasets\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = 'facial-keypoints-detection.zip'\n",
    "extract_path = 'datasets'\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "if os.path.exists(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(f\"Successfully unzipped {zip_path} to {extract_path}\")\n",
    "else:\n",
    "    print(f\"Error: {zip_path} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e36b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully unzipped datasets/test.zip to datasets\n",
      "Successfully unzipped datasets/training.zip to datasets\n"
     ]
    }
   ],
   "source": [
    "test_zip = \"datasets/test.zip\"\n",
    "training_zip = \"datasets/training.zip\"\n",
    "\n",
    "for zip_file in (test_zip, training_zip):\n",
    "    if os.path.exists(zip_file):\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        print(f\"Successfully unzipped {zip_file} to {extract_path}\")\n",
    "    else:\n",
    "        print(f\"Error: {zip_file} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96ecb802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>182 183 182 182 180 180 176 169 156 137 124 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>76 87 81 72 65 59 64 76 69 42 31 38 49 58 58 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>177 176 174 170 169 169 168 166 166 166 161 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>176 174 174 175 174 174 176 176 175 171 165 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50 47 44 101 144 149 120 58 48 42 35 35 37 39 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId                                              Image\n",
       "0        1  182 183 182 182 180 180 176 169 156 137 124 10...\n",
       "1        2  76 87 81 72 65 59 64 76 69 42 31 38 49 58 58 4...\n",
       "2        3  177 176 174 170 169 169 168 166 166 166 161 14...\n",
       "3        4  176 174 174 175 174 174 176 176 175 171 165 15...\n",
       "4        5  50 47 44 101 144 149 120 58 48 42 35 35 37 39 ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_path = \"datasets/training.csv\"\n",
    "test_path = \"datasets/test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# train_df.head(3)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1eb691d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "left_eye_center_x            0\n",
       "left_eye_center_y            0\n",
       "right_eye_center_x           0\n",
       "right_eye_center_y           0\n",
       "left_eye_inner_corner_x      0\n",
       "left_eye_inner_corner_y      0\n",
       "left_eye_outer_corner_x      0\n",
       "left_eye_outer_corner_y      0\n",
       "right_eye_inner_corner_x     0\n",
       "right_eye_inner_corner_y     0\n",
       "right_eye_outer_corner_x     0\n",
       "right_eye_outer_corner_y     0\n",
       "left_eyebrow_inner_end_x     0\n",
       "left_eyebrow_inner_end_y     0\n",
       "left_eyebrow_outer_end_x     0\n",
       "left_eyebrow_outer_end_y     0\n",
       "right_eyebrow_inner_end_x    0\n",
       "right_eyebrow_inner_end_y    0\n",
       "right_eyebrow_outer_end_x    0\n",
       "right_eyebrow_outer_end_y    0\n",
       "nose_tip_x                   0\n",
       "nose_tip_y                   0\n",
       "mouth_left_corner_x          0\n",
       "mouth_left_corner_y          0\n",
       "mouth_right_corner_x         0\n",
       "mouth_right_corner_y         0\n",
       "mouth_center_top_lip_x       0\n",
       "mouth_center_top_lip_y       0\n",
       "mouth_center_bottom_lip_x    0\n",
       "mouth_center_bottom_lip_y    0\n",
       "Image                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#결측치 확인\n",
    "train_df.isnull().sum()\n",
    "\n",
    "# 바로 앞 행의 데이터로 결측치 채우기\n",
    "train_df.fillna(method=\"ffill\", inplace=True)\n",
    "# test_df.fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d9787e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 데이터 형태 (7049, 96, 96)\n",
      "이미지 데이터 형태 (1783, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "# 이미지로 변환\n",
    "def num_to_img(data):\n",
    "    face_images = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        image_str = data['Image'][i].split(' ')\n",
    "        image_int = np.array(image_str, dtype=np.uint8).reshape((96, 96))\n",
    "\n",
    "        face_images.append(image_int)\n",
    "\n",
    "    X = np.array(face_images)\n",
    "\n",
    "    print(\"이미지 데이터 형태\", X.shape)\n",
    "\n",
    "    return X\n",
    "    \n",
    "\n",
    "x_train = num_to_img(train_df)\n",
    "x_test = num_to_img(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aac87156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블 데이터 형태: (7049, 30)\n",
      "레이블 데이터 형태: (1783, 1)\n"
     ]
    }
   ],
   "source": [
    "# 'Image' 컬럼을 제외하고 나머지 키포인트 컬럼들만 선택합니다.\n",
    "def get_label(data):\n",
    "    y_original = data.drop(['Image'], axis=1).values\n",
    "\n",
    "    scale_factor = 224 / 96\n",
    "    y_scaled = y_original * scale_factor\n",
    "\n",
    "    y_normalized = (y_scaled - 112) / 112\n",
    "\n",
    "    y = y_normalized\n",
    "\n",
    "    print(\"레이블 데이터 형태:\", y.shape)\n",
    "    \n",
    "    return y\n",
    "\n",
    "y_train = get_label(train_df)\n",
    "y_test = get_label(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22260ab3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7049, 5639]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-86cb5fbe1915>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"google/vit-base-patch16-224-in21k\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minja\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2170\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"At least one array required as input\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2172\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2174\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minja\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \"\"\"\n\u001b[0;32m    298\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minja\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    263\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7049, 5639]"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "train_dataset = datasets.Dataset.from_dict({\"pixel_values\": X_train, \"labels\": y_train})\n",
    "val_dataset = datasets.Dataset.from_dict({\"pixel_values\": X_val, \"labels\": y_val})\n",
    "\n",
    "\n",
    "def transform(example):\n",
    "    image = np.array(example['pixel_values'])\n",
    "    image_rgb = np.stack([image] * 3, axis=-1)\n",
    "\n",
    "    # 프로세서로 이미지 가공 (리사이징, 정규화, 텐서 변환)\n",
    "    inputs = processor(images=image_rgb, return_tensors=\"pt\")\n",
    " \n",
    "    # 가공된 이미지 텐서로 'pixel_values' 업데이트\n",
    "    example['pixel_values'] = inputs['pixel_values'].squeeze(0)\n",
    "\n",
    "    # 레이블을 float 타입의 텐서로 변환\n",
    "    example['labels'] = torch.tensor(example['labels']).float()\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "train_dataset.set_transform(transform)\n",
    "val_dataset.set_transform(transform)\n",
    "\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=30,\n",
    "    problem_type=\"regression\",\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./facial-keypoints-results\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\", # 매 에폭마다 검증 수행\n",
    "    save_strategy=\"epoch\",       # 매 에폭마다 모델 저장\n",
    "    load_best_model_at_end=True, # 훈련 종료 후 최적 모델 로드\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# Trainer: 모델, 설정, 데이터셋을 받아 훈련을 자동화하는 클래스\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# # 훈련 시작\n",
    "# trainer.train()\n",
    "# print(\"훈련 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcfc4c80",
   "metadata": {},
   "outputs": [
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"요청한 작업은, 사용자가 매핑한 구역이 열려 있는 상태인 파일에서 수행할 수 없습니다.\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d9a0d1468799>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"face_model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"face_model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minja\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[0;32m   3027\u001b[0m                 \u001b[1;31m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3028\u001b[0m                 \u001b[1;31m# joyfulness), but for now this enough.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3029\u001b[1;33m                 \u001b[0msafe_save_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshard_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"format\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3030\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3031\u001b[0m                 \u001b[0msave_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshard_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minja\\anaconda3\\lib\\site-packages\\safetensors\\torch.py\u001b[0m in \u001b[0;36msave_file\u001b[1;34m(tensors, filename, metadata)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m     \"\"\"\n\u001b[1;32m--> 286\u001b[1;33m     \u001b[0mserialize_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSafetensorError\u001b[0m: Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"요청한 작업은, 사용자가 매핑한 구역이 열려 있는 상태인 파일에서 수행할 수 없습니다.\" })"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"face_model\")\n",
    "processor.save_pretrained(\"face_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71366579",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-56508bd3fff1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Get predictions from the trainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mpredicted_keypoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets # Assuming 'datasets' from Hugging Face was imported earlier\n",
    "\n",
    "# Assuming 'x_test', 'y_test', 'transform', 'trainer', and 'train_df' are defined earlier in the script.\n",
    "\n",
    "# Load the fine-tuned model and processor\n",
    "model_path = \"face_model\"\n",
    "processor = ViTImageProcessor.from_pretrained(model_path)\n",
    "model = ViTForImageClassification.from_pretrained(model_path)\n",
    "\n",
    "# Create and transform the test dataset\n",
    "test_dataset = datasets.Dataset.from_dict({\"pixel_values\": x_test, \"labels\": y_test})\n",
    "test_dataset.set_transform(transform)\n",
    "\n",
    "# Get predictions from the trainer\n",
    "predictions = trainer.predict(test_dataset)\n",
    "predicted_keypoints = predictions.predictions\n",
    "\n",
    "def visualize_predictions(images, predicted_keypoints, num_samples=5):\n",
    "    \"\"\"\n",
    "    Visualizes images with their predicted facial keypoints.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(num_samples):\n",
    "        ax = plt.subplot(1, num_samples, i + 1)\n",
    "        keypoints = predicted_keypoints[i] * 112 + 112\n",
    "        keypoints = keypoints.reshape(-1, 2)\n",
    "        \n",
    "        # Display the image (assuming it's a 2D grayscale array)\n",
    "        plt.imshow(images[i].reshape(224, 224), cmap='gray')\n",
    "        plt.scatter(keypoints[:, 0], keypoints[:, 1], c='red', marker='x')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize some predictions\n",
    "visualize_predictions(x_test, predicted_keypoints)\n",
    "\n",
    "# Prepare data for submission file\n",
    "id_lookup_table = pd.read_csv('datasets/IdLookupTable.csv')\n",
    "feature_names = list(id_lookup_table['FeatureName'])\n",
    "image_ids = list(id_lookup_table['ImageId'])\n",
    "row_ids = list(id_lookup_table['RowId'])\n",
    "\n",
    "# Create a mapping from feature name to its column index in the original training data\n",
    "feature_name_to_index = {name: i for i, name in enumerate(train_df.columns[:-1])}\n",
    "\n",
    "# Extract the specific predicted keypoints required by the submission format\n",
    "locations = []\n",
    "for feature_name, image_id in zip(feature_names, image_ids):\n",
    "    image_index = image_id - 1\n",
    "    feature_index = feature_name_to_index[feature_name]\n",
    "    \n",
    "    # Denormalize the predicted keypoint coordinate\n",
    "    # This should match the denormalization used for training/visualization\n",
    "    location = (predicted_keypoints[image_index, feature_index] * 112) + 112\n",
    "    locations.append(location)\n",
    "\n",
    "# Create and save the submission DataFrame\n",
    "submission_df = pd.DataFrame({'RowId': row_ids, 'Location': locations})\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab8ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
